2020-10-15:
	Created repo.
	Added the code from https://github.com/xinghai-sun/deep-rl.
  	Installed the required dependencies.
  	Tweaked pong_env.py so that you can run only that and play against a decent opponent with the up and down keys. Added powerups.
  	There were a lot of bugs, for example when the ball hit the roof and stayed there, had to change a lot regarding the velocity for that to work.
  	Tried running "python run_async.py --config conf/a3c_my-pong.yaml", but there were a lot of bugs here as well. All of the code is written in Python 2, which to some extent does not work in Python 3.
  	Didn't get multiprocessing to work.
  	Didn't get the AtariRescale42x42Wrapper to work.
  	Finally started the first training.
	Example of changes from Python2 to Python3: xrange -> range, file() -> open() + add Loader=Loader with 'from yaml import CLoader as Loader'
	The input (h, c) to 'forward' in 'a3c.py' seems a bit strange. Handled it like a single input instead of tuple and divided it into h and c afterwards
  	TODO: Use GPU instead

2020-10-16:
	Managed to get the network to run on GPU, if present.
	Got multiprocessing working. Had to move 'learn_thread' and 'create_env' out of 'run_async', and fix the respective inputs.
	Got AtariRescale42x42Wrapper to work, had to remove '_' from the two observation methods in 'process_frame'.
	Got evaluation to start working.
	Started the recordings in training and evaluation, had to run 'conda install -c conda-forge ffmpeg'.
	Changed 'reset_round', in 'Bat', in 'pong_env.py' so that the bats gets recentered in the middle at every round. This should affect the training greatly.
	TODO: Get self-play working, get a decent agent trained, add powerups
	
2020-10-19
	Trained a A3C agent during the weekend, 10k episodes on 4 processes, with recording.
	Checked the recordings, they seem alright even though it is not a brilliant player yet.
	Got self-play working, had to make changes like in the previous files. Trained for 1k episodes.
	Not possible to record self-play training, or at least we do not know how to fix the bug.
	Started writing on the report.
	Started evaluating different levels of the trained agents against former versions of itself and the AutoBat.
	
2020-10-20:
	Continued writing the report.
	Trained the agent with self-play for another 2k episodes.
	Evaluated the performance against the previous trained agents.
	Added a single powerup (small bat).
	Started doing the poster.
	
2020-10-21:
	Trained the agent for another 1k episodes, this time with the "smaller bat" power-up
	Evaluated the trained agent against previous versions
	Finished up the report and poster
